{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30b31c01",
   "metadata": {},
   "source": [
    "# Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9e11d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, LayerNormalization, MultiHeadAttention, TimeDistributed, Masking, Layer\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39828488",
   "metadata": {},
   "source": [
    "# Positional embedding class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f400363",
   "metadata": {},
   "outputs": [],
   "source": [
    "#class PositionalEncoding:\n",
    "class PositionalEncoding(Layer):\n",
    "    def __init__(self):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "    def get_angles(self, pos, i, d_model):\n",
    "        angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
    "        return pos * angle_rates\n",
    "\n",
    "    def call(self, seq_len, d_model):\n",
    "        #seq_len = inputs.shape[1]\n",
    "        #d_model = inputs.shape[2]\n",
    "        angles = self.get_angles(np.arange(seq_len)[:, np.newaxis], np.arange(d_model)[np.newaxis, :], d_model)\n",
    "        angles[:, 0::2] = np.sin(angles[:, 0::2])\n",
    "        angles[:, 1::2] = np.cos(angles[:, 1::2])\n",
    "        pos_encoding = angles[np.newaxis, ...]\n",
    "        return tf.cast(pos_encoding, tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55bb308",
   "metadata": {},
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c05d163",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, mask, dropout=0):\n",
    "    # Self-attention\n",
    "    attention, scores = MultiHeadAttention(num_heads=num_heads, key_dim=head_size)(inputs, inputs, attention_mask=mask,\n",
    "                                                                                   return_attention_scores=True)\n",
    "    attention = Dropout(dropout)(attention)\n",
    "    out1 = LayerNormalization(epsilon=1e-6)(inputs + attention)\n",
    "    \n",
    "    # Feed-forward network\n",
    "    ffn = Dense(ff_dim, activation='relu')(out1)\n",
    "    ffn = Dense(inputs.shape[-1])(ffn)\n",
    "    ffn = Dropout(dropout)(ffn)\n",
    "    ffn = LayerNormalization(epsilon=1e-6)(out1 + ffn)\n",
    "    ffn = Dense(ff_dim)(ffn)\n",
    "    return ffn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065ee453",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_attention_block(inputs_a, inputs_b, head_size, num_heads, dropout=0, mask_a=None, mask_b=None):\n",
    "    # Cross-attention from A to B\n",
    "    attention_ab, score_a = MultiHeadAttention(num_heads=num_heads, key_dim=head_size)(\n",
    "        inputs_a, inputs_b, attention_mask=mask_a, return_attention_scores=True)\n",
    "    attention_ab = Dropout(dropout)(attention_ab)\n",
    "    out_ab = LayerNormalization(epsilon=1e-6)(inputs_a + attention_ab)\n",
    "    \n",
    "    # Cross-attention from B to A\n",
    "    attention_ba, score_b = MultiHeadAttention(num_heads=num_heads, key_dim=head_size)(\n",
    "        inputs_b, inputs_a, attention_mask=mask_b, return_attention_scores=True)\n",
    "    attention_ba = Dropout(dropout)(attention_ba)\n",
    "    out_ba = LayerNormalization(epsilon=1e-6)(inputs_b + attention_ba)\n",
    "    \n",
    "    return out_ab, out_ba, score_a, score_b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ef70ab",
   "metadata": {},
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df58e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_decoder(inputs, encoder_outputs, head_size, num_heads, ff_dim, dropout=0):#, encoder_mask=None):\n",
    "    # Masked self-attention (causal attention)\n",
    "    attention = MultiHeadAttention(num_heads=num_heads, key_dim=head_size)\n",
    "    attention_out = attention(inputs, inputs, use_causal_mask=True)\n",
    "    attention_out = Dropout(dropout)(attention_out)\n",
    "    out1 = LayerNormalization(epsilon=1e-6)(inputs + attention_out)\n",
    "    \n",
    "    # Cross-attention\n",
    "    cross_attention = MultiHeadAttention(num_heads=num_heads, key_dim=head_size)\n",
    "    attention_out = cross_attention(out1, encoder_outputs)#, attention_mask=encoder_mask)\n",
    "    attention_out = Dropout(dropout)(attention_out)\n",
    "    out2 = LayerNormalization(epsilon=1e-6)(out1 + attention_out)\n",
    "    \n",
    "    # Feed-forward\n",
    "    ffn = Dense(ff_dim, activation='relu')\n",
    "    ffn_out = ffn(out2)\n",
    "    ffn_out = Dense(inputs.shape[-1])(ffn_out)\n",
    "    ffn_out = Dropout(dropout)(ffn_out)\n",
    "    out3 = LayerNormalization(epsilon=1e-6)(out2 + ffn_out)\n",
    "    \n",
    "    return out3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b99b21f",
   "metadata": {},
   "source": [
    "# Build Cross-attentional Transformer-AutoRegressive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a5fc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_transformer_model(max_seq_length, dim_a, dim_b, head_size, num_heads, ff_dim, dropout=0):\n",
    "    pe = PositionalEncoding()\n",
    "    # Encoder input\n",
    "    encoder_input_a = Input(shape=(max_seq_length, dim_a))\n",
    "    encoder_input_a += pe(max_seq_length, dim_a)\n",
    "    encoder_input_b = Input(shape=(max_seq_length, dim_b))\n",
    "    encoder_input_b += pe(max_seq_length, dim_b)\n",
    "    \n",
    "    # Masking to handle variable length sequences and features\n",
    "    time_step_mask_a = tf.math.not_equal(encoder_input_a[:, :, 0], -50.0)\n",
    "    time_step_mask_a = tf.cast(time_step_mask_a[:, None, None, :], tf.float32)\n",
    "    time_step_mask_b = tf.math.not_equal(encoder_input_b[:, :, 0], -50.0)\n",
    "    time_step_mask_b = tf.cast(time_step_mask_b[:, None, None, :], tf.float32)\n",
    "    \n",
    "    # Encoder\n",
    "    encoder_output_a = encoder_input_a\n",
    "    encoder_output_b = encoder_input_b\n",
    "    \n",
    "    # Self-attention\n",
    "    encoder_output_a = transformer_encoder(encoder_output_a, head_size, num_heads, ff_dim, time_step_mask_a, dropout)\n",
    "    encoder_output_b = transformer_encoder(encoder_output_b, head_size, num_heads, ff_dim, time_step_mask_b, dropout)\n",
    "    \n",
    "    # Cross-attention between modalities\n",
    "    cross_a, cross_b, cross_score_a, cross_score_b = cross_attention_block(encoder_output_a, encoder_output_b, head_size, num_heads, dropout,\n",
    "                                                                           time_step_mask_a, time_step_mask_b)\n",
    "\n",
    "    # Concatenate the Cross-attention outputs\n",
    "    combined = tf.concat([cross_a, cross_b], axis=2)\n",
    "    combined = Dense(ff_dim, activation='relu')(combined)\n",
    "    combined = Dropout(dropout)(combined)\n",
    "    encoder_output = LayerNormalization(epsilon=1e-6)(combined)\n",
    "    \n",
    "    # Decoder (predicting next two time points)\n",
    "    # Decoder input \n",
    "    decoder_inputs = Input(shape=(2, dim_a+dim_b))\n",
    "    decoder_inputs += pe(2, dim_a+dim_b)\n",
    "    decoder_outputs = transformer_decoder(decoder_inputs, encoder_output, head_size, num_heads, ff_dim, dropout)\n",
    "    \n",
    "    # Output layer\n",
    "    outputs = TimeDistributed(Dense(dim_a+dim_b))(decoder_outputs)\n",
    "    \n",
    "    # CAAT-EHR Model\n",
    "    model = Model([encoder_input_a, encoder_input_b, decoder_inputs], outputs)\n",
    "    # CAAT-EHR encoder Model\n",
    "    encoder = Model([encoder_input_a, encoder_input_b], encoder_output, name='encoder')\n",
    "    \n",
    "    #return model, score_a, score_b, cross_score_a, cross_score_b\n",
    "    return model, encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc7f15b",
   "metadata": {},
   "source": [
    "# Retrieve the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b94242d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unpikle data\n",
    "file_name = 'modal1.pkl'\n",
    "X_pretrain_modal1 = pd.read_pickle(file_name)\n",
    "\n",
    "file_name = 'modal2.pkl'\n",
    "X_pretrain_modal2 = pd.read_pickle(file_name)\n",
    "\n",
    "file_name = 'target.pkl'\n",
    "pretrain_target = pd.read_pickle(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1fab35",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pretrain_modal1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c44cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pretrain_modal2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea639dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_target.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b39dfe5-d13a-4ee4-9aff-2d5b3905189d",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43558a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "MAX_SEQ_LENGTH = 20  # Maximum sequence length\n",
    "INPUT_DIM_1 = 15\n",
    "INPUT_DIM_2 = 30\n",
    "HEAD_SIZE = 32\n",
    "NUM_HEADS = 2\n",
    "FF_DIM = 64\n",
    "DROPOUT_RATE = 0.1\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 100\n",
    "LEARNING_RATE = 1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e621c1d-d93c-4130-8fe3-a33d5613d442",
   "metadata": {},
   "source": [
    "# Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773f975a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and compile the model\n",
    "model, encoder_model = build_transformer_model(MAX_SEQ_LENGTH, INPUT_DIM_1, INPUT_DIM_2,HEAD_SIZE, NUM_HEADS, FF_DIM, DROPOUT_RATE)\n",
    "optimizer = Adam(learning_rate=LEARNING_RATE)\n",
    "model.compile(optimizer=optimizer, loss='mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901c8eb0-9b05-431f-bc4b-d0519ba8f8ec",
   "metadata": {},
   "source": [
    "# Pre-train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b78ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "callback = keras.callbacks.EarlyStopping(monitor='loss', patience=20, mode=\"min\")\n",
    "history = model.fit([X_pretrain_modal1, X_pretrain_modal2, pretrain_target], (pretrain_target), batch_size=BATCH_SIZE, epochs=EPOCHS,\n",
    "                    callbacks=[callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514daa5e",
   "metadata": {},
   "source": [
    "# Save and load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58721d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model.save('transformer_encoder.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9557dc0-2f8e-4c10-ae3c-88b1e4d25ded",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:miniconda3]",
   "language": "python",
   "name": "conda-env-miniconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
